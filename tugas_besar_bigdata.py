# -*- coding: utf-8 -*-
"""Tugas Besar BigData.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mOUFR23Nhy9o3WrfKPu5ATXMSsD3qAXk

#CRAWL YOUTUBE
"""

import pandas as pd
from googleapiclient.discovery import build

def video_comments(video_id):
	# empty list for storing reply
	replies = []

	# creating youtube resource object
	youtube = build('youtube', 'v3', developerKey=api_key)

	# retrieve youtube video results
	video_response = youtube.commentThreads().list(part='snippet,replies', videoId=video_id).execute()

	# iterate video response
	while video_response:

		# extracting required info
		# from each result object
		for item in video_response['items']:

			# Extracting comments ()
			published = item['snippet']['topLevelComment']['snippet']['publishedAt']
			user = item['snippet']['topLevelComment']['snippet']['authorDisplayName']

			# Extracting comments
			comment = item['snippet']['topLevelComment']['snippet']['textDisplay']
			likeCount = item['snippet']['topLevelComment']['snippet']['likeCount']

			replies.append([published, user, comment, likeCount])

			# counting number of reply of comment
			replycount = item['snippet']['totalReplyCount']

			# if reply is there
			if replycount>0:
				# iterate through all reply
				for reply in item['replies']['comments']:

					# Extract reply
					published = reply['snippet']['publishedAt']
					user = reply['snippet']['authorDisplayName']
					repl = reply['snippet']['textDisplay']
					likeCount = reply['snippet']['likeCount']

					# Store reply is list
					#replies.append(reply)
					replies.append([published, user, repl, likeCount])

			# print comment with list of reply
			#print(comment, replies, end = '\n\n')

			# empty reply list
			#replies = []

		# Again repeat
		if 'nextPageToken' in video_response:
			video_response = youtube.commentThreads().list(
					part = 'snippet,replies',
					pageToken = video_response['nextPageToken'],
					videoId = video_id
				).execute()
		else:
			break
	#endwhile
	return replies

"""Run Crawl"""

# isikan dengan api key
api_key = 'AIzaSyAKenCf09wcI1cBGhqwoVC9dlmct_4NBwM'

# Enter video id
video_id = "wW1lY5jFNcQ" #isikan dengan kode / ID video

# Call function
comments = video_comments(video_id)


"""Ubah ke dataframe"""

df = pd.DataFrame(comments, columns=['publishedAt', 'authorDisplayName', 'textDisplay', 'likeCount'])

pip install "pymongo[srv]" #hanya dijalankan sekali, hanya untuk menginstall modul pymongo

from pymongo.mongo_client import MongoClient
from pymongo.server_api import ServerApi

uri = "mongodb+srv://rayhankrnwn:atlas789@cluster0.hws32xd.mongodb.net/?retryWrites=true&w=majority&appName=AtlasApp"

# Create a new client and connect to the server
client = MongoClient(uri, server_api=ServerApi('1'))

# Send a ping to confirm a successful connection
try:
    client.admin.command('ping')
    print("Pinged your deployment. You successfully connected to MongoDB!")
except Exception as e:
    print(e)

# Pilih database dan koleksi
db = client.get_database("DBTubes")
collection = db.get_collection("CrawlYT")

# Ubah DataFrame menjadi bentuk dokumen BSON (Python dictionary)
data_to_insert = df.to_dict(orient='records')

# Masukkan data ke koleksi MongoDB Atlas
# collection.insert_many(data_to_insert)
for data in data_to_insert:
    # Tentukan kriteria pencarian berdasarkan field tertentu yang unik (misalnya 'publishedAt')
    filter_criteria = {"publishedAt": data["publishedAt"],
                       "authorDisplayName": data["authorDisplayName"],
                       "textDisplay": data["textDisplay"]}

    # Perbarui dokumen yang cocok atau buat dokumen baru jika tidak ada yang cocok
    collection.update_one(filter_criteria, {"$set": data}, upsert=True)
print("Impor data selesai")

"""#CRAWL TWITTER"""

twitter_auth_token = '772986f0d109bdc554d4714e7e5aed33a40459e9'

# Install Node.js (because tweet-harvest built using Node.js)
!sudo apt-get update
!sudo apt-get install -y ca-certificates curl gnupg
!sudo mkdir -p /etc/apt/keyrings
!curl -fsSL https://deb.nodesource.com/gpgkey/nodesource-repo.gpg.key | sudo gpg --dearmor -o /etc/apt/keyrings/nodesource.gpg

!NODE_MAJOR=20 && echo "deb [signed-by=/etc/apt/keyrings/nodesource.gpg] https://deb.nodesource.com/node_$NODE_MAJOR.x nodistro main" | sudo tee /etc/apt/sources.list.d/nodesource.list

!sudo apt-get update
!sudo apt-get install nodejs -y

!node -v

# Crawl Data
search_keyword = 'Manchester United lang:en'
limit = 50
filename = 'MU.csv'

!npx --yes tweet-harvest@2.2.8 -o "{filename}" -s "{search_keyword}" -l {limit} --token {twitter_auth_token}

import pandas as pd

# Baca file CSV ke dalam DataFrame
file_path = f"tweets-data/{filename}"
df = pd.read_csv(file_path, delimiter=";")
print(f"Jumlah tweet dalam dataframe adalah {len(df)}.")

from pymongo.mongo_client import MongoClient
from pymongo.server_api import ServerApi

# MongoDB connection URI and Client Setup
uri = "mongodb+srv://rayhankrnwn:atlas789@cluster0.hws32xd.mongodb.net/?retryWrites=true&w=majority&appName=AtlasApp"
client = MongoClient(uri, server_api=ServerApi('1'))
db = client.get_database("DBTubes")
collection = db.get_collection("CrawlTwitter")

# Fungsi untuk menyimpan data ke MongoDB
def save_to_mongodb(df, collection):
    # Ubah DataFrame menjadi dictionary
    data_dict = df.to_dict('records')
    try:
        # Masukkan semua data ke MongoDB
        collection.insert_many(data_dict)
        print(f"Data inserted successfully, {len(data_dict)} records added.")
    except Exception as e:
        print("An error occurred:", e)
    finally:
        client.close()

# Simpan data ke MongoDB
save_to_mongodb(df, collection)

"""#MENGAMBIL FILE MONGO KE JUPYTER NOTEBOOK"""

from pymongo import MongoClient

# Ganti dengan string koneksi yang benar.
connection_string = "mongodb+srv://rayhankrnwn:atlas789@cluster0.hws32xd.mongodb.net/DBTubes?retryWrites=true&w=majority"

# Membuat klien MongoDB.
client = MongoClient(connection_string)

# Pilih database MongoDB Anda.
db = client.DBTubes

# Pilih koleksi yang akan di-query.
collection = db.CrawlYT

# Membuat query untuk memfilter dan mendapatkan hanya field `full_text`.
query = {}  # Query kosong akan mengembalikan semua dokumen.
projection = {'_id': 0, 'textDisplay': 1}  # Proyeksi untuk field `full_text` saja.

# Melakukan query ke koleksi.
results = collection.find(query, projection)

# Membuat DataFrame dari hasil query.
df = pd.DataFrame(list(results))

# Menyimpan DataFrame ke file CSV.
csv_file_path = 'sentiment_texts.csv'
df.to_csv(csv_file_path, index=False, columns=['textDisplay'])

print(f"File CSV telah disimpan di: {csv_file_path}")

"""# ANALISIS SENTIMEN DENGAN SPARK"""

!pip install spark-nlp
!pip install pyspark

import sparknlp

# Start Spark Session
spark = sparknlp.start()

! wget -N https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/resources/en/lemma-corpus-small/lemmas_small.txt -P /tmp
! wget -N https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/resources/en/sentiment-corpus/default-sentiment-dict.txt -P /tmp

# Import the required modules and classes
from sparknlp.base import DocumentAssembler, Pipeline, Finisher
from sparknlp.annotator import (
    SentenceDetector,
    Tokenizer,
    Lemmatizer,
    SentimentDetector
)
import pyspark.sql.functions as F

# Step 1: Transforms raw texts to document annotation
document_assembler = (
    DocumentAssembler()
    .setInputCol("text")
    .setOutputCol("document")
)

# Step 2: Sentence Detection
sentence_detector = SentenceDetector().setInputCols(["document"]).setOutputCol("sentence")

# Step 3: Tokenization
tokenizer = Tokenizer().setInputCols(["sentence"]).setOutputCol("token")

# Step 4: Lemmatization
lemmatizer = Lemmatizer().setInputCols(["token"]).setOutputCol("lemma").setDictionary("/tmp/lemmas_small.txt", key_delimiter="->", value_delimiter="\t")

# Step 5: Sentiment Detection
sentiment_detector = (
    SentimentDetector()
    .setInputCols(["lemma", "sentence"])
    .setOutputCol("sentiment_score")
    .setDictionary("/tmp/default-sentiment-dict.txt", ",")
)

# Step 6: Finisher
finisher = (
    Finisher()
    .setInputCols(["sentiment_score"])
    .setOutputCols(["sentiment"])
)

# Define the pipeline
pipeline = Pipeline(
    stages=[
        document_assembler,
        sentence_detector,
        tokenizer,
        lemmatizer,
        sentiment_detector,
        finisher
    ]
)

from pyspark.sql.functions import lower, regexp_replace, trim

# Membaca file CSV ke dalam DataFrame Spark.
csv_file_path = 'sentiment_texts.csv'
data = spark.read.option("header", "true").csv(csv_file_path)

if 'textDisplay' in data.columns:
    data = data.withColumnRenamed("textDisplay", "text")
# Memastikan bahwa kolom 'text' bertipe string untuk pengolahan lebih lanjut.
data = data.withColumn("text", data["text"].cast("string"))

# Preprocessing pada kolom 'text'
# Mengubah teks menjadi lowercase
data = data.withColumn("text", lower(data["text"]))

# Menghilangkan angka
data = data.withColumn("text", regexp_replace(data["text"], r'\d+', ''))

# Menghapus tanda baca
data = data.withColumn("text", regexp_replace(data["text"], r'[^\w\s]', ''))

# Menghapus whitespace di awal dan akhir string
data = data.withColumn("text", trim(data["text"]))

# Menampilkan hasil preprocessing
data.show(5, truncate=50)

# Fit-transform untuk mendapatkan prediksi dengan menggunakan pipeline yang sudah ditentukan.
result = pipeline.fit(data).transform(data)

# Menampilkan hasil prediksi.
result.show(n=result.count(), truncate=False)

from pyspark.sql.functions import explode

exploded_result = result.withColumn("sentiment", explode(F.col("sentiment")))
sentiment_counts = exploded_result.groupBy("sentiment").count()
sentiment_counts_list = sentiment_counts.collect()

# Plotting with matplotlib.
import matplotlib.pyplot as plt

# Prepare data for plotting.
sentiment_dict = {row['sentiment']: row['count'] for row in sentiment_counts_list}
categories = sentiment_dict.keys()
counts = sentiment_dict.values()

# Create bar chart.
plt.bar(categories, counts)
plt.title('Sentiment Counts')
plt.xlabel('Sentiment')
plt.ylabel('Count')
plt.show()

from pyspark.sql.functions import col
from pymongo import MongoClient
import pandas as pd

result = result.withColumn("sentiment", col("sentiment").getItem(0))

# Melanjutkan dengan proses konversi ke Pandas DataFrame dan mengunggah ke MongoDB seperti sebelumnya.
pandas_df = result.select("text", "sentiment").toPandas()

# Konfigurasi koneksi MongoDB.
connection_string = "mongodb+srv://rayhankrnwn:atlas789@cluster0.hws32xd.mongodb.net/DBTubes?retryWrites=true&w=majority"
client = MongoClient(connection_string)

# Pilih database dan koleksi di MongoDB Atlas.
db = client["DBTubes"]
collection = db["Hasil_Sentimen"]

# Mengunggah data ke MongoDB Atlas.
collection.insert_many(pandas_df.to_dict('records'))

print("Data berhasil diunggah ke MongoDB Atlas.")